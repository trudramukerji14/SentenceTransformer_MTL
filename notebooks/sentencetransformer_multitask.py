# -*- coding: utf-8 -*-
"""SentenceTransformer_MultiTask.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E-Pi_kbkLe6qtk4q_fn0UYsie93BWJPF

In this notebook, we will implement a sentence transformer using PyTorch and HuggingFace. We will first demonstrate an implementation with sample sentences, then will leverage the yelp dataset to perform multi-task learning, which allows models to share parameters between different tasks. We will write the class for doing so. We will discuss various design choices along the way.

In order to load the yelp dataset we will datasets installed.
"""

!pip install datasets

"""We import the following."""

from transformers import DistilBertModel, DistilBertTokenizer
import torch
import torch.nn as nn
import os
import random
import logging

from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader

"""#Implement a Sentence Transformer

We will now implement a sentence transformer model that can convert input sentences into fixed-length vector embeddings.

In this notebook, we select [DistilBERT](https://arxiv.org/pdf/1910.01108) as our transformer backbone. A language model that is 40% smaller then BERT and 60% faster while still retaining 97% of its language reasoning capabilities.
"""

transformer_backbone = DistilBertModel.from_pretrained('distilbert-base-uncased')
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

"""We write a class which intializes our chosen tokenizer and transformer as a sentence transformer that outputs vector embeddings."""

class SentenceTransformer(nn.Module):
    def __init__(self, transformer_model, tokenizer):
      super(SentenceTransformer, self).__init__()
      self.transformer = transformer_model
      self.tokenizer = tokenizer

    def forward(self, sentence):
      print(f'sentence: {sentence}')
      input_ids = self.tokenizer(sentence, return_tensors='pt')
      attention_mask = input_ids['attention_mask']
      input_ids = input_ids['input_ids']
      self.transformer.eval()
      transformer_output = self.transformer(input_ids = input_ids, attention_mask = attention_mask)
      output = transformer_output.last_hidden_state.detach().numpy()
      return output

"""We now demonstrate the outputs of"""

sentences = ['This pizza was good', 'I like ducks', 'ducks like bread']

for sentence in sentences:
  input_ids = tokenizer(sentence, return_tensors='pt')
  model = SentenceTransformer(transformer_backbone, tokenizer)
  output = model(sentence)
  print(f'vector output: {output}')

"""#Multi-Task Model:

Sentence transformers such as BERT and DistilBert can be used for a variety of downstream tasks ranging from sentiment classification or [reading comprehension](https://arxiv.org/abs/1606.05250).

We can encorporate separate tasks in a single model which allows the distinct tasks to share parameters.

In this notebook, we will focus on the [yelp-review dataset](https://business.yelp.com/data/resources/open-dataset/) available on Huggingface's [datasets](https://huggingface.co/datasets/Yelp/yelp_review_full).

##Loading and Processing the Dataset
"""

from datasets import load_dataset

yelp_ds = load_dataset("Yelp/yelp_review_full")

#visualizing a few samples of the dataset:
train_dataset = yelp_ds['train']
for i in range(10):
    sample = train_dataset[i]
    print(f"--- Sample {i+1} ---")
    print(f"Text: {sample['text']}")
    print(f"Label: {sample['label']}")

"""We will now begin to pre-process the dataset.



1.   We convert the text to lower case characters.
2.   We will then classify the text into three sentiment categories "positive," "neutral," and "negative." This is based the label indicated by the review.

We use the [map function](https://huggingface.co/docs/datasets/en/process) function to apply our processing function to each example within a specific batch.

"""

#function to convert text to lowercase

def lowercase_function(examples):
    return {"text": [t.lower() for t in examples["text"]]}

#apply lowercase_function to yelp.ds
yelp_ds = yelp_ds.map(lowercase_function, batched=True)



#let's relabel the items of dataset as positive, neutral, and negative:
def sentiment_label(review):
  if review > 3:
    return 0 #positive
  elif review == 3:
    return 1 #neutral
  else:
    return 2 #negative

#processing function on a batch based on sentiment_label
def sentiment_function(examples):
    return {"label": [sentiment_label(label) for label in examples["label"]]}

#apply sentiment_function to yelp.ds
yelp_labeled = yelp_ds.map(sentiment_function, batched=True)

"""We will also need a processing function specific to one of our tasks. We will consider the task of detecting a keyword within the text.


This can be roughly thought of as whether a particular review is disussing a restaurant or not. We will discuss the choice of tasks below. But for now, let's continue with the processing.

We need to add a label to the dataset corresponding to whether the text contains one of the food keywords.
"""

food_keywords = ["food", "delicious", "menu", "tasty", "dish", "cuisine", "eat", "restaurant", "hot sauce", "wings", "pizza", "flavor", "sandwich", "fish", "chicken"]


def contains_food_keyword_whole_word(t):
  """Checks if the review text contains any food-related keywords as whole words."""
  text_lower = t.lower() #lower case text if not already
  words = text_lower.split()
  keywords_lower = [keyword for keyword in food_keywords]
  valid_words = [''.join(char for char in word if char.isalpha()) for word in words] #consider whole words consisting of just alphabet characters
  found = any(word in keywords_lower for word in valid_words)
  return 1 if found else 0

#processing function
def contains_food_keyword_function(examples):
  return {"contains_food_keyword": [contains_food_keyword_whole_word(t) for t in examples['text']]}

#apply processing function to yelp_labeled
food_labeled_yelp = yelp_labeled.map(contains_food_keyword_function, batched=True)

"""We can again visualize a few samples of our processed dataset."""

train_dataset = food_labeled_yelp['train']

for i in range(10):
    sample = train_dataset[i]
    print(sample)
    print(f"--- Sample {i+1} ---")
    print(f"Text: {sample['text']}")
    text = sample['text']
    print(f"Label: {sample['label']}")
    print(f"Contains Food Keyword: {sample['contains_food_keyword']}")

"""As part of the processing, we also need to apply our chosen tokenizer.

This process can be time-consuming on the entire dataset which contains 65,000 training examples and 10,000 test examples.

The following codebox gives the option to work with smaller subsets of the dataset for fast experimentation.

**Alternatively, one could apply the tokenization to the entire dataset and save the processed dataset. Then one need only apply the tokenization once.**
"""

# Create a subset
num_train_samples = 2000
num_test_samples = 400
train_indices = random.sample(range(len(food_labeled_yelp["train"])), num_train_samples)
small_train_dataset = food_labeled_yelp['train'].select(train_indices)
small_test_dataset = food_labeled_yelp["test"].select(range(num_test_samples)) # Using slicing for test

"""We can also split the test set obtain a validation set for training."""

split = small_test_dataset.train_test_split(test_size=0.5, shuffle=False, seed=42)

small_validation_dataset = split["train"]
small_test_dataset = split["test"]

print("Validation dataset size:", len(small_validation_dataset))
print("Final test dataset size:", len(small_test_dataset))

"""We now apply the tokenizer"""

# Load the DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenization function
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

# Apply tokenization to the subset
tokenized_small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
tokenized_small_validation_dataset = small_validation_dataset.map(tokenize_function, batched=True)
tokenized_small_test_dataset = small_test_dataset.map(tokenize_function, batched=True)

"""Creating dataloaders"""

#set format
tokenized_small_train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "contains_food_keyword", "label"])
tokenized_small_test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "contains_food_keyword", "label"])
tokenized_small_validation_dataset.set_format("torch", columns=["input_ids", "attention_mask", "contains_food_keyword", "label"])

#create dataloaders
small_train_dataloader = DataLoader(tokenized_small_train_dataset, batch_size=10, shuffle=True)
small_test_dataloader = DataLoader(tokenized_small_test_dataset, batch_size=16, shuffle=False)
small_validation_dataloader = DataLoader(tokenized_small_validation_dataset, batch_size=16, shuffle=False)

"""## Description of Tasks

We focus on the following two tasks:

1.   Key-word Detection: This task wants to detect whether a text contains one of the key-words.



```
food_keywords = ["food", "delicious", "menu", "tasty", "dish", "cuisine", "eat", "restaurant", "hot sauce", "wings", "pizza", "flavor", "sandwich", "fish", "chicken"]

```
The processing above, labels the elements of the dataset as "1" if one of these key-words is found and "0" otherwise. This provides us with a binary classification task to consider. This task is in some sense is a over-simplication of the comprehension task of detecting whether a review is a restaurant review or not.

Unfortunately the yelp-dataset that we loaded doesn't come with such labels. So one would "roughly" label reviews based on whether it contains the above mentioned key-words that are commonly associated with eating at a restaurant.

2.   Sentiment Analysis

This task simply aims to see if the sentence transformer can understand whether the text from a review falls in one of three different categories ("0"-positive, "1"-neutral, "2"-negative).

##The Model Class

We now write our model as the following class.

The model consists of


1.   A transformer backbone: This is what we use to convert the token embeddings into vector representations as seen above.
2.   A classifier head: This is a feedforward neural network with the task of classifying text that contains the food keywords.

3. A sentiment head for classifying sentiment class of the text in the review.
"""

class MultiTaskModel(nn.Module):
  def __init__(self, transformer_model, num_classes, num_sentiments):
    super(MultiTaskModel, self).__init__()
    self.transformer = transformer_model
    self.classifier_head = nn.Linear(self.transformer.config.hidden_size, num_classes) #head for classifying whether text contains food keywords
    self.sentiment_head = nn.Linear(self.transformer.config.hidden_size, num_sentiments) #head for classifying sentiment

  def forward(self, input_ids, attention_mask):
    transformer_output = self.transformer(input_ids = input_ids, attention_mask = attention_mask)
    classifier_logits = self.classifier_head(transformer_output.last_hidden_state[:, 0, :])
    sentiment_logits = self.sentiment_head(transformer_output.last_hidden_state[:, 0, :])
    return classifier_logits, sentiment_logits

"""##Setting the Model

We can now set the model for the what follows.
"""

transformer_model = DistilBertModel.from_pretrained('distilbert-base-uncased')
num_classes = 2
num_sentiments = 3
multitask_model = MultiTaskModel(transformer_model, num_classes=num_classes, num_sentiments = num_sentiments)

"""# Training Considerations

We now discuss training considerations for this model.

##Freezing Layers.

Our model consists of a transformer model and two FNNs corresponding to the classifier and sentiment heads.

We get a count of the number of trainable parameters with the following codebox.
"""

total_trainable_params = 0

for name, param in multitask_model.named_parameters():
  if param.requires_grad:
    num_params = param.numel()
    total_trainable_params += num_params
    print(f"{name}: {num_params}")  # Optional: Print the number of parameters for each trainable layer

print(f"\nTotal Trainable Parameters: {total_trainable_params}")

"""Running this code should give us a count of over 65 million trainable parameters!

This actually comes mostly from the transformer backbone. DistilBERT has 65 million trainable parameters.

Rather than train all of these parameters, we can freeze the parameters coming from DistilBERT and focus only on updating the weights corresponding the classifier and sentiment heads respectively.

We can freeze layers of our model using the following function.
"""

def freeze_layers(model, freeze_transformer = True, freeze_classifier = False, freeze_sentiment_classifier = False):
  for name, param in model.named_parameters():
    if freeze_transformer and "transformer" in name:
      param.requires_grad = False
    if freeze_classifier and "classifier_head" in name:
      param.requires_grad = False
    if freeze_sentiment_classifier and "sentiment_head" in name:
      param.requires_grad = False

#apply the freeze layer function to the transformer backbone
freeze_layers(multitask_model)

"""We can compare the number of parameters after freezing DistilBERT. We see that we have 3800 parameters to train roughly for the classifer and sentiment classifer heads.

We could have also frozen either one fo the two tasks but chose not to do so.
"""

total_trainable_params = 0

for name, param in multitask_model.named_parameters():
  if param.requires_grad:
    num_params = param.numel()
    total_trainable_params += num_params
    print(f"{name}: {num_params}")  # Optional: Print the number of parameters for each trainable layer

print(f"\nTotal Trainable Parameters: {total_trainable_params}")

"""##Summary

In summary, we have chosen to do a form of transfer learning where we have chosen DistilBERT as our pre-trained layer and starting point. We choose this as our starting point due to its relatively small-size and reasoning capabilities.

We choose to freeze the over 65 million trainable parameters of DistillBERT. This will allows us to specifically train the feedforward layers corresponding the tasks of food keyword classification and sentiment analysis. These tasks have a small number of parameters and provide an illustrative small scale example.

Another consideration not explored in this notebook is quantization. There exist a variety of methods of quantization such as QLORA and LORA to reduce memory requirements while fine-tuning large models.

# Training the Model

We start by setting a random seed.
"""

import random
import numpy as np

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

"""We repeat the code above for setting the model if needed"""

transformer_model = DistilBertModel.from_pretrained('distilbert-base-uncased')
num_classes = 2
num_sentiments = 3
multitask_model = MultiTaskModel(transformer_model, num_classes=num_classes, num_sentiments = num_sentiments)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
multitask_model.to(device)
freeze_layers(multitask_model)

"""We now write out the functions needed to execute the training of our model.

The function will include the following arguments which we set prior to running the model.



1.   Optimizer: This allows us to choose which algorithm or version of gradient descent to update the model parameters. We choose Adam which has been [demonstrated](https://proceedings.neurips.cc/paper_files/paper/2024/file/ee0e45ff4de76cbfdf07015a7839f339-Paper-Conference.pdf) to outperform sgd for transformer based models. We also choose a learning rate as a hyperparameter. We use a general learning rate for the all the tasks to keep things simple.

2. We choose our device in the event a GPU is available.

3. We also pre-set the loss function. Since this is a classification problem, we use the standard cross entropy for both tasks. In what follows, we compute a loss $L_{class}$ for the class head and another loss $L_{sentiment}$ for the sentiment head. We then compute the total loss as the average
$$L_{total} = (\alpha_{1}L_{sentiment}+ \alpha_{2}L_{class})$$
where $\alpha_{1} + \alpha_{2} = 1$

4. A writer for logging.
"""

optimizer = torch.optim.Adam(multitask_model.parameters(), lr=1e-3)
loss_fn = nn.CrossEntropyLoss()
log_dir = "runs"
os.makedirs(log_dir, exist_ok=True)
writer = SummaryWriter(log_dir=log_dir)
max_iter = 10000

"""##Validation Loss

We now define a validate function which can be used for early stopping. This function computes the average validation loss given from the validation dataset.
"""

# Define the validate function
def validate_model(model, dataloader, loss_fn, device):
    model.eval()
    total_loss = 0
    total = 0

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            class_labels = batch["contains_food_keyword"].to(device)
            sentiment_labels = batch["label"].to(device)

            class_output, sentiment_output = model(input_ids, attention_mask)
            class_loss = loss_fn(class_output, class_labels)
            sentiment_loss = loss_fn(sentiment_output, sentiment_labels)
            loss = 0.3 * class_loss + 0.7* sentiment_loss

            total_loss += loss.item() * class_labels.size(0)
            total += class_labels.size(0)

    return total_loss / total  # Return average validation loss

"""##Training Function

We now write the function that executes the training of the model. For the purpose of illustration, we write out th


This function


1.   Sets the model to training mode
2.   For each epoch, loads each batch of the training dataloader and updates the parameters with each batch. For every 10 iterations, we log the accuracy of our model with respect to food class predictions and sentiment predictions.
3.  After each epoch, we compute the validation loss. We stop training if we see no improvement in the validation loss after a preset number of epochs we call patience.

We found it more descriptive to write out the training loop with PyTorch rather then relying on a high-level API which are available via HuggingFace or PyTorch Lightning.


Metrics: We use accuracy for the


"""

# Update the training function
def multitask_train(model, train_dataloader, val_dataloader, loss_fn, optimizer, device, writer, max_iter= 10000, max_epochs=10, patience=5):
    model.train()

    iter = 0
    correct_class_pred = 0
    correct_sentiment_pred = 0
    total = 0

    best_val_loss = float('inf')
    epochs_without_improvement = 0

    for epoch in range(max_epochs):
        # Training loop
        for batch in train_dataloader:
            if iter >= max_iter:
                print('Max iterations reached')
                return

            input_ids = batch["input_ids"].to(device) #inputs
            attention_mask = batch["attention_mask"].to(device)
            class_labels = batch["contains_food_keyword"].to(device)
            sentiment_labels = batch["label"].to(device)

            optimizer.zero_grad() #zero gradients

            #forward pass
            class_output, sentiment_output = model(input_ids, attention_mask)
            class_loss = loss_fn(class_output, class_labels)
            sentiment_loss = loss_fn(sentiment_output, sentiment_labels)
            loss = 0.3 * class_loss + 0.7 * sentiment_loss

            total += class_labels.size(0)

            # Compute predictions and accuracy
            class_predicted = torch.argmax(class_output, dim=1)
            correct_class_pred += (class_predicted == class_labels).sum().item()

            sentiment_predicted = torch.argmax(sentiment_output, dim=1)
            correct_sentiment_pred += (sentiment_predicted == sentiment_labels).sum().item()

            #backpropagation
            loss.backward()
            optimizer.step()


            if iter % 20 == 0:
                class_accuracy = correct_class_pred / total
                sentiment_accuracy = correct_sentiment_pred / total

                print(f'Iter {iter}, Loss: {loss.item():.4f}, Class Acc: {class_accuracy:.4f}, Sentiment Acc: {sentiment_accuracy:.4f}')

                if iter % 100 == 0:
                    # Logging to TensorBoard
                    writer.add_scalar('Accuracy/Class', class_accuracy, iter)
                    writer.add_scalar('Accuracy/Sentiment', sentiment_accuracy, iter)
                    writer.add_scalar('Loss/Total', loss.item(), iter)
            iter += 1

        # After each epoch, validate the model and check for early stopping
        val_loss = validate_model(model, val_dataloader, loss_fn, device)

        print(f"Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}")

        # Early stopping logic
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_without_improvement = 0
            # Save the best model
            torch.save(model.state_dict(), 'best_model.pt')
        else:
            epochs_without_improvement += 1

        # If no improvement for 'patience' epochs, stop training
        if epochs_without_improvement >= patience:
            print(f"Early stopping triggered after {epoch + 1} epochs.")
            break

    writer.close()
    print('Training complete')

"""It may be helpful to check if we can overfit a single batch before running the whole train function.

We now run the training function.
"""

multitask_train(multitask_model, small_train_dataloader, small_validation_dataloader, loss_fn, optimizer, device, writer, max_iter)

"""##Visualizing Results

We can potentially visualize our results using tensorboard. More time could of been spent on making these visualizations more illustrative but they should communicate that the model is learning.
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir runs

"""##Debugging

Initially when running the training loop above, we ran into an issue where our training loss wasn't decreasing. In order to investigate this, we implemented a function to overfit on a single batch while allowing us to observe norms of the gradients being communicated during backpropagation and the resulting update used by the optimizer.
"""

def test_overfit_one_batch(model, dataloader, loss_fn, optimizer, device, num_steps=100):

    model.train()
    batch = next(iter(dataloader))
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    class_labels = batch["contains_food_keyword"].to(device)
    sentiment_labels = batch["label"].to(device)

    for step in range(num_steps):
        optimizer.zero_grad()

        class_output, sentiment_output = model(input_ids, attention_mask)



        class_loss = loss_fn(class_output, class_labels)
        sentiment_loss = loss_fn(sentiment_output, sentiment_labels)
        loss = 0.3 * class_loss + 0.7 * sentiment_loss
        loss.backward()

        update_sizes = {}

        # Save a copy of params before update
        for name, param in model.named_parameters():
            if param.requires_grad:
                #print(f"[GRAD] {name} | grad norm: {param.grad.norm().item():.6f}") #for the norms of the gradients used in BP
                update_sizes[name] = param.data.clone()

        # Do the optimizer step
        optimizer.step()

        # Compare params after update
        for name, param in model.named_parameters():
            if name in update_sizes:
                update = (param.data - update_sizes[name]).norm().item() #compute norm of update
                print(f"[UPDATE] {name} | update norm: {update:.6f}")

        # Metrics
        class_preds = torch.argmax(class_output, dim=1)
        sentiment_preds = torch.argmax(sentiment_output, dim=1)
        class_acc = (class_preds == class_labels).float().mean().item()
        sentiment_acc = (sentiment_preds == sentiment_labels).float().mean().item()

        print(f"Step {step + 1}, Loss: {loss.item():.4f}, Class Acc: {class_acc:.4f}, Sentiment Acc: {sentiment_acc:.4f}")

"""##Summary

In this section, we implemented and discussed a function for training our model. This involved computing the forward pass and backpropagating gradients to update parameters using the Adam optimizer with a unified learning rate for simplicity across our two tasks. We tracked training success through the loss and accuracy of both the sentiment and class heads. Furthermore, we discussed the practical technique of observing overfitting on a single batch to confirm the model's ability to learn.
"""