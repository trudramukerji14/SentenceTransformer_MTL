Here we implement the sentence transformer DistilBERT and use it as the transformer backbone for multi-task learning. 



**Motivation**

Many NLP tasks share underyling language understanding for various tasks. By implementing and training a model on different tasks, allows the model to share parameters and benefit from a potentially richer understanding.

The demo is also available to run on colab.
| Model       | Colab link |
|:------------|-------------|

| Demo:    | [link](https://colab.research.google.com/drive/1E-Pi_kbkLe6qtk4q_fn0UYsie93BWJPF?usp=sharing) |





