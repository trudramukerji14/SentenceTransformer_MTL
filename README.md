Here we implement the sentence transformer DistilBERT and use it as the transformer backbone for multi-task learning. 



**Motivation**

Many NLP tasks share underyling language understanding for various tasks. By implementing and training a model on different tasks, allows the model to share parameters and benefit from a potentially richer understanding.





